# Question generator ai agent

## Author
- **Name**: Debtanu Das   
- **University**: IIT Bhubaneswar
- **Department**: Computer Science

---

## ğŸ“Œ Project Overview
This project builds an **AI Agent** that can **reason, plan, and execute** to automate a manual academic task:  
**Summarization and Question Generation from Scientific Documents.**

The agent uses:
- A **fine-tuned large language model (Qwen3-Next-80B-Instruct with LoRA adapters)** for **scientific summarization**.  
- **Gemini Flash API** for generating and answering questions from the processed documents.  
- **LangChain + Chroma** for retrieval and vector database storage.  

---

## ğŸ¯ Why This Fine-Tuning Target?
I chose **scientific summarization** as the fine-tuning task because:
- Summarizing research papers and long technical documents is **time-consuming** in daily university work.  
- Fine-tuning ensures **task specialization**: the model produces **concise, domain-adapted summaries**.  
- Improves **reliability**: generic LLMs often hallucinate; the fine-tuned model stays faithful to input text.  
- Enables **adapted style**: outputs match the academic abstract style expected in reports and research summaries.  

---

## ğŸ—ï¸ Agent Architecture
**Components & Flow:**
1. **Document Loader** â€“ Load PDF lecture notes/reports using `PyPDFLoader`.  
2. **Vector Store** â€“ Store embeddings with Chroma for semantic search.  
3. **Fine-Tuned Model** â€“ `finetune.py` trains Qwen3 with LoRA on arXiv summarization dataset.  
4. **Question Generator** â€“ `question_generator.py` generates exam-style questions.  
5. **Answer Generator** â€“ Retrieves context, summarizes it, and produces answers.  
6. **Evaluator** â€“ Uses ROUGE/BLEU metrics to evaluate agent reliability.  

---

## ğŸ“‚ Repository Contents
- `finetune.py` â†’ Fine-tuning setup (Qwen + LoRA on arXiv dataset).  
- `question_generator.py` â†’ AI Agent logic (question generation + answering).  
- `result.txt` â†’ Generated Q&A outputs.  
- `Process_QA` used to evaluate the mertics

---

## ğŸ“Š Data Science Report

### Fine-Tuning Setup
- **Dataset**: [kaggle dataset](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail)  
- **Base Model**: facebook/bart-base  
- **Quantization**: 4-bit (bitsandbytes) for GPU efficiency  
- **Training**: 3 epoch, learning rate 3e-5  

### Results
- Model outputs short, abstract-style summaries.  
- Trainable parameters greatly reduced using LoRA (~0.1% of base model).  

### Evaluation
We used **ROUGE metrics** to measure summarization quality:  

- **ROUGE-1**: Precision = 0.387, Recall = 0.778, F1 = 0.517  
- **ROUGE-2**: Precision = 0.184, Recall = 0.369, F1 = 0.245  
- **ROUGE-L**: Precision = 0.212, Recall = 0.427, F1 = 0.284  


---

## ğŸš€ Deliverables
- âœ… Source code of the prototype (`finetune.py`, `question_generator.py`)  
- âœ… AI agent architecture documentation (this README)  
- âœ… Data science report (fine-tuning setup, evaluation with ROUGE scores)  

---






